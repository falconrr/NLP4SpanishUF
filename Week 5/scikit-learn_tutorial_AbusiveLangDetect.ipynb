{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"scikit-learn_tutorial_AbusiveLangDetect.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kTdrav3GJAPh"},"source":["# **Práctica 1**: Detección de lenguaje abusivo en Twitter\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SiPLlcEs-UUn"},"source":["Este tutorial muestra un modelo basado en machine learning que permite la detección del lenguaje abusivo en Twitter, tomando como base los datos recogidos por Waseem and Hovy (2016). "]},{"cell_type":"code","metadata":{"id":"AO6ZrIokRjDo"},"source":["#opcional: puedes subir los datos o montar tu Drive\n","#from google.colab import drive\n","#drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W0kitIM4-oj3"},"source":["Importamos algunos de los paquetes necesarios para este tutorial. Utilizaremos la libreria de módulos de Python **SciKitLearn**. "]},{"cell_type":"code","metadata":{"id":"Kqz8PKJRQKJ-","executionInfo":{"status":"ok","timestamp":1627345043115,"user_tz":300,"elapsed":1099,"user":{"displayName":"Falcon Restrepo Ramos","photoUrl":"","userId":"01792138933088691349"}}},"source":["import numpy as np\n","import pandas as pd \n","\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","\n","!pip install sklearn\n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mMJ97oCgRIlJ","executionInfo":{"status":"ok","timestamp":1627345105333,"user_tz":300,"elapsed":46616,"user":{"displayName":"Falcon Restrepo Ramos","photoUrl":"","userId":"01792138933088691349"}},"outputId":"a6ef6b41-1296-4739-cb7e-d98e10a69246"},"source":["# Tal como lo hicimos la clase pasada, podemos instalar e importar el módulo de preprocesamiento para tuits.\n","!pip install nlpretext\n","from nlpretext.social.preprocess import remove_emoji\n","from nlpretext.social.preprocess import remove_hashtag\n","from nlpretext.social.preprocess import remove_mentions\n","from nlpretext.basic.preprocess import replace_urls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting nlpretext\n","  Downloading nlpretext-1.0.4-py3-none-any.whl (92 kB)\n","\u001b[K     |████████████████████████████████| 92 kB 227 kB/s \n","\u001b[?25hCollecting nlpaug==1.0.1\n","  Downloading nlpaug-1.0.1-py3-none-any.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 29.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>1.15.4 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (1.19.5)\n","Collecting regex==2019.8.19\n","  Downloading regex-2019.08.19.tar.gz (654 kB)\n","\u001b[K     |████████████████████████████████| 654 kB 41.1 MB/s \n","\u001b[?25hCollecting mosestokenizer==1.1.0\n","  Downloading mosestokenizer-1.1.0.tar.gz (37 kB)\n","Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (3.0.4)\n","Collecting spacy==2.3.4\n","  Downloading spacy-2.3.4-cp37-cp37m-manylinux2014_x86_64.whl (10.4 MB)\n","\u001b[K     |████████████████████████████████| 10.4 MB 2.7 MB/s \n","\u001b[?25hCollecting nltk>=3.4.5\n","  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 44.4 MB/s \n","\u001b[?25hCollecting flashtext==2.7\n","  Downloading flashtext-2.7.tar.gz (14 kB)\n","Collecting sacremoses==0.0.13\n","  Downloading sacremoses-0.0.13.tar.gz (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 55.5 MB/s \n","\u001b[?25hCollecting stop-words==2018.7.23\n","  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n","Collecting phonenumbers==8.10.12\n","  Downloading phonenumbers-8.10.12-py2.py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 40.7 MB/s \n","\u001b[?25hCollecting emoji>=0.5.2\n","  Downloading emoji-1.4.1.tar.gz (185 kB)\n","\u001b[K     |████████████████████████████████| 185 kB 41.5 MB/s \n","\u001b[?25hCollecting ftfy<5.0.0,>=4.2.0\n","  Downloading ftfy-4.4.3.tar.gz (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.4 MB/s \n","\u001b[?25hCollecting scikit-learn==0.23.2\n","  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 43.7 MB/s \n","\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from mosestokenizer==1.1.0->nlpretext) (0.6.2)\n","Collecting openfile\n","  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n","Collecting uctools\n","  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n","Collecting toolwrapper\n","  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.13->nlpretext) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.13->nlpretext) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.13->nlpretext) (1.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.13->nlpretext) (4.41.1)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->nlpretext) (1.4.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->nlpretext) (57.2.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->nlpretext) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->nlpretext) (3.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->nlpretext) (2.23.0)\n","Collecting thinc<7.5.0,>=7.4.1\n","  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 42.3 MB/s \n","\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->nlpretext) (1.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->nlpretext) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->nlpretext) (1.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->nlpretext) (2.0.5)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->nlpretext) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.4->nlpretext) (0.8.2)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.4->nlpretext) (4.6.1)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.7/dist-packages (from ftfy<5.0.0,>=4.2.0->nlpretext) (1.0.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy<5.0.0,>=4.2.0->nlpretext) (0.2.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.4->nlpretext) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.4->nlpretext) (3.7.4.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.4->nlpretext) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.4->nlpretext) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.4->nlpretext) (1.24.3)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib->ftfy<5.0.0,>=4.2.0->nlpretext) (0.5.1)\n","Building wheels for collected packages: flashtext, mosestokenizer, regex, sacremoses, stop-words, emoji, ftfy, toolwrapper, uctools\n","  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9309 sha256=976182c1097249f38fad0da28cc7a1941c97e735f58c5386c8a2bfd19d64432f\n","  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n","  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mosestokenizer: filename=mosestokenizer-1.1.0-py3-none-any.whl size=49119 sha256=b8a4fda1c1d8e46f2ae345977fef5f451ba8469bb90a684b233f1d61704677ff\n","  Stored in directory: /root/.cache/pip/wheels/a7/31/94/fef279382208e85a65c1a7f5c4d0020115477b0af74f296b57\n","  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for regex: filename=regex-2019.8.19-cp37-cp37m-linux_x86_64.whl size=609788 sha256=a9e65e81b43dbec576de03a6d138f858748951663d66e477f4195193336eaa02\n","  Stored in directory: /root/.cache/pip/wheels/28/8f/a2/6a273ec4395fdf35dc0fcb842e6f32a0f8f65190f5f0cbe5ad\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.13-py3-none-any.whl size=154729 sha256=e7ecc032dc675f10c958e0e14730d82837330b09c8715559c28ae774feeb54df\n","  Stored in directory: /root/.cache/pip/wheels/03/38/9f/ae9fce563ce84aead39e1b7893ac7cbb3428c24ebea487eb52\n","  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32911 sha256=0066f1715d76794b8975931365dd62e05198d09da2833f897ce55bea06166689\n","  Stored in directory: /root/.cache/pip/wheels/fb/86/b2/277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.4.1-py3-none-any.whl size=186393 sha256=b686189e22e2b7bcda92225b0929b8a4795d1ea057886d408179c901472403c4\n","  Stored in directory: /root/.cache/pip/wheels/8e/68/ac/537456a5331f1405779f2b3c2a578429d2f6d7419e440330d8\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-4.4.3-py3-none-any.whl size=41082 sha256=d10eda69982afc77a1a8178e2c85747088eeb7df69dfeae3b3119d030cdb2b0d\n","  Stored in directory: /root/.cache/pip/wheels/b0/66/08/c65b9e8a3b674f10739790db0cbbc846afaa20a3f80f0b9e42\n","  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3353 sha256=b07e19c9965d586ead5c734ffefde0144833e50600578bc20a4bec75b3cd3913\n","  Stored in directory: /root/.cache/pip/wheels/c5/4f/33/54741ffe08e38ececb1d28068a153729b4fe820bafa0a0691f\n","  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6161 sha256=ee17b6cec3336cd5a72caf73911299172c9e0fca1ab907d25f3ea8283c6bbc0e\n","  Stored in directory: /root/.cache/pip/wheels/fb/44/e9/914cf8fa71f0141f9314f862538d1218fcf2b94542a0fb7d35\n","Successfully built flashtext mosestokenizer regex sacremoses stop-words emoji ftfy toolwrapper uctools\n","Installing collected packages: uctools, toolwrapper, threadpoolctl, thinc, regex, openfile, stop-words, spacy, scikit-learn, sacremoses, phonenumbers, nltk, nlpaug, mosestokenizer, ftfy, flashtext, emoji, nlpretext\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed emoji-1.4.1 flashtext-2.7 ftfy-4.4.3 mosestokenizer-1.1.0 nlpaug-1.0.1 nlpretext-1.0.4 nltk-3.6.2 openfile-0.0.7 phonenumbers-8.10.12 regex-2019.8.19 sacremoses-0.0.13 scikit-learn-0.23.2 spacy-2.3.4 stop-words-2018.7.23 thinc-7.4.5 threadpoolctl-2.2.0 toolwrapper-2.1.0 uctools-1.3.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["sklearn"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"XH4FT4JWTJPZ","executionInfo":{"status":"ok","timestamp":1627323016789,"user_tz":-120,"elapsed":785,"user":{"displayName":"Santiago Arroniz Parra","photoUrl":"","userId":"14616025295717850812"}},"outputId":"9baf7ef1-94ef-4d0c-e559-7375a0a023bb"},"source":["# opcional: aquí verificamos el directorio en el que estamos\n","import os\n","os.getcwd()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"naZRpKi8G5Qc"},"source":["Asignamos variables para los dos sets, train y test. Los datos para este tutorial están divididos en varios archivos de texto."]},{"cell_type":"code","metadata":{"id":"FP1rfzTJQdWB"},"source":["import pandas as pd\n","import numpy as np\n","\n","data_tweets_train = open('/content/drive/MyDrive/ML_Florida_Tutorial/Data/Waseem/waseemtrain.txt').read()\n","data_labels_train = open('/content/drive/MyDrive/ML_Florida_Tutorial/Data/Waseem/waseemtrainGold.txt').read()\n","\n","data_tweets_test = open('/content/drive/MyDrive/ML_Florida_Tutorial/Data/Waseem/waseemtest.txt').read()\n","data_labels_test = open('/content/drive/MyDrive/ML_Florida_Tutorial/Data/Waseem/waseemtestGold.txt').read()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"04YjgWddHC3U"},"source":["Limpiamos los datos quitando lo que no nos interese, como emojis, hashtags, urls... Podemos utilizar nuestra función de la clase pasada o hacerlo manualmente mediante *for loops* como aquí."]},{"cell_type":"code","metadata":{"id":"e6AUeNMGQgYo"},"source":["tweets_train = []\n","labels_train = []\n","\n","tweets_test = []\n","labels_test = []\n","\n","for line in data_tweets_train.split(\"\\n\"):\n","    line = remove_emoji(line)\n","    line = remove_hashtag(line)\n","    line = remove_mentions(line)\n","    line = replace_urls(line, \"\")\n","    tweets_train.append(line)\n","    \n","for label in data_labels_train.split(\"\\n\"):\n","    labels_train.append(label)\n","    \n","\n","for line in data_tweets_test.split(\"\\n\"):\n","    line = remove_emoji(line)\n","    line = remove_hashtag(line)\n","    line = remove_mentions(line)\n","    line = replace_urls(line, \"\")\n","    tweets_test.append(line)\n","    \n","    \n","for label in data_labels_test.split(\"\\n\"):\n","    labels_test.append(label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bGCKHkASHUTX"},"source":["Aquí tenemos algunos ejemplos para ambos sets:"]},{"cell_type":"code","metadata":{"id":"M5uF8askQiWV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627323541096,"user_tz":-120,"elapsed":651,"user":{"displayName":"Santiago Arroniz Parra","photoUrl":"","userId":"14616025295717850812"}},"outputId":"7440d31e-f88d-44bc-c940-f2dbdc4567a4"},"source":["# 1 = abusive; 2 = non-abusive\n","print(tweets_train[23], labels_train[23])\n","print(tweets_train[12394], labels_train[12394])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Come and get your Jizya scumbag. I have it waiting in 0.4 cal copper and lead. 1\n","Read about the Muslim invasion of India from historian Will Durant. 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9R7iPirEQkRe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627323582896,"user_tz":-120,"elapsed":322,"user":{"displayName":"Santiago Arroniz Parra","photoUrl":"","userId":"14616025295717850812"}},"outputId":"42108633-a2e6-466a-a65e-1d9614e2dc38"},"source":["# 1 = abusive; 2 = non-abusive\n","print(tweets_test[23], labels_test[23])\n","print(tweets_test[754], labels_test[754])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Can we kill for Don? And does he give us a bunch of virgins in heaven for doing it? 1\n","RT : Bianca is feeling sick. She tried the baked Greek eggs. 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TQjtdEhiHb95"},"source":["Ahora crearemos un dataframe con los datos que hemos limpiado usando Pandas. "]},{"cell_type":"code","metadata":{"id":"ZSREbnvqQoKb"},"source":["corpus = pd.DataFrame()\n","corpus['tweet'] = tweets_train\n","corpus['label'] = labels_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmRYdczGHn-o"},"source":["Como vemos, el corpus está compuesto por 14143 filas (todos los tweets) y dos columnas (texts y labels)."]},{"cell_type":"code","metadata":{"id":"ga1-MAyzQ3VJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627323617343,"user_tz":-120,"elapsed":401,"user":{"displayName":"Santiago Arroniz Parra","photoUrl":"","userId":"14616025295717850812"}},"outputId":"adab4879-828b-4062-ed09-d3d8aae2506a"},"source":["print('Shape of train set {}'.format(corpus.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of train set (14143, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U96JwfwXH5iq"},"source":["Este es el balance con respecto a las labels. "]},{"cell_type":"code","metadata":{"id":"avuo1dBJREsS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627323636121,"user_tz":-120,"elapsed":547,"user":{"displayName":"Santiago Arroniz Parra","photoUrl":"","userId":"14616025295717850812"}},"outputId":"68ab6c50-2ac4-478a-f1ee-c06437855da1"},"source":["corpus['label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2    9683\n","1    4460\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"hQTy5uWmo3zQ"},"source":["\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iWcFBhoMICX3"},"source":["Ahora que tenemos nuestros datos limpios y ordenados en un dataframe, podemos empezar a experimentar con diferentes métodos de modelaje estadístico. En este tutorial, usaremos solo un modelo (Support Vector Classification o SVC), pero jugaremos un poco con los features (variables o labels) y parameters (optimización del módelo). También puedes intentar probar con otros modelos estadísticos, como Naive Bayes, Random Forest, etc. (https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)"]},{"cell_type":"markdown","metadata":{"id":"Xd7GTEK9ozoP"},"source":["Parametros: **Unigrams**"]},{"cell_type":"markdown","metadata":{"id":"5sF-krOoIz9G"},"source":["Vamos a empezar usando unigrams para nuestro modelo. "]},{"cell_type":"code","metadata":{"id":"uAjXbIybo0Rr"},"source":["from sklearn.svm import SVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84Bi_0WmI6P-"},"source":["scikit-learn solo funciona con vectores, no con palabras, por lo que primero necesitamos convertir nuestros datos en vectores para después entrenar el modelo. "]},{"cell_type":"code","metadata":{"id":"WS6_Ejzmo922"},"source":["svc = Pipeline([\n","        (\"count_vectorizer\", CountVectorizer(analyzer = 'word', \n","                                             ngram_range = (1,1),\n","                                             token_pattern=r'\\b\\w+\\b', \n","                                             min_df=1)),\n","        (\"linear svc\", SVC(kernel=\"rbf\")) # try with \"linear\", \"poly\", \"rbf\" or \"sigmoid\"\n","    ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MWGy9pO5pA04","executionInfo":{"status":"ok","timestamp":1627323968238,"user_tz":-120,"elapsed":60374,"user":{"displayName":"Santiago Arroniz Parra","photoUrl":"","userId":"14616025295717850812"}},"outputId":"9b5ec87f-fee7-4a73-e9d8-d02ad12dedce"},"source":["model = svc\n","model.probability = True\n","model.fit(tweets_train, labels_train)\n","y_pred = model.predict(tweets_test)\n","print(accuracy_score(labels_test, y_pred))\n","print(classification_report(labels_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8150031786395423\n","              precision    recall  f1-score   support\n","\n","                   0.00      0.00      0.00         1\n","           1       0.79      0.56      0.66       496\n","           2       0.82      0.93      0.87      1076\n","\n","    accuracy                           0.82      1573\n","   macro avg       0.54      0.50      0.51      1573\n","weighted avg       0.81      0.82      0.80      1573\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"KaHYTDEPJNmu"},"source":["¿Cómo leer la tabla?   \n","**Precision**: the ratio of correctly predicted positive observations to the total predicted positive observations.   \n","**Recall**: the ratio of correctly predicted positive observations to the all observations in actual class - yes.   \n","**F1-score**: the weighted average of Precision and Recall.    "]},{"cell_type":"markdown","metadata":{"id":"vaZ1-WHFKR_I"},"source":["También podemos usar oraciones para ver la predicción asignada por el modelo. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJpONgrkpFte","executionInfo":{"status":"ok","timestamp":1627324044811,"user_tz":-120,"elapsed":452,"user":{"displayName":"Santiago Arroniz Parra","photoUrl":"","userId":"14616025295717850812"}},"outputId":"255c1067-21be-40cc-dd3e-b8b6f0141c3d"},"source":["sentence = (['Islam is worse than the Nazi party ever was.'])\n","label_prediction = model.predict(sentence)\n","print(label_prediction) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["['1']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zBv9NYsPpGdr","executionInfo":{"status":"ok","timestamp":1627324054289,"user_tz":-120,"elapsed":537,"user":{"displayName":"Santiago Arroniz Parra","photoUrl":"","userId":"14616025295717850812"}},"outputId":"938984ca-80fa-4f52-b9e4-ecc26fff1d3c"},"source":["sentence = (['Islamic State Executes 5 Men In Mosul After Their Wives Fail To Wear New “Afghan-Style” Hijab…'])\n","label_prediction = model.predict(sentence)\n","print(label_prediction) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["['2']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0hsnTQ60paIW"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"slSBrtmsKaBu"},"source":["Ahora, para intentar mejorar nuestro modelo, podemos intentar con bigrams:"]},{"cell_type":"markdown","metadata":{"id":"QjVimGiGpa37"},"source":["Parametros: **Bigrams**\n"]},{"cell_type":"code","metadata":{"id":"INzfnxZPpbX-"},"source":["svc = Pipeline([\n","        (\"count_vectorizer\", CountVectorizer(analyzer = 'word',\n","                                             ngram_range=(2, 2), \n","                                             token_pattern=r'\\b\\w+\\b', \n","                                             min_df=1)),\n","        (\"linear svc\", SVC(kernel=\"rbf\")) # try with \"linear\", \"poly\", \"rbf\" or \"sigmoid\"\n","    ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFe5e1ALuAFD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627324196736,"user_tz":-120,"elapsed":76079,"user":{"displayName":"Santiago Arroniz Parra","photoUrl":"","userId":"14616025295717850812"}},"outputId":"db865c6d-6fde-4c3a-f34f-93461bf7c8a9"},"source":["model = svc\n","model.probability = True\n","model.fit(tweets_train, labels_train)\n","y_pred = model.predict(tweets_test)\n","print(accuracy_score(labels_test, y_pred))\n","print(classification_report(labels_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7768595041322314\n","              precision    recall  f1-score   support\n","\n","                   0.00      0.00      0.00         1\n","           1       0.87      0.35      0.50       496\n","           2       0.76      0.98      0.86      1076\n","\n","    accuracy                           0.78      1573\n","   macro avg       0.54      0.44      0.45      1573\n","weighted avg       0.80      0.78      0.74      1573\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MgplMThaKjye"},"source":["La precisión en general ha bajado. Probemos con un modelo de trigrams:"]},{"cell_type":"markdown","metadata":{"id":"iUDrp-CtuEoq"},"source":["Parametros: **Trigrams**"]},{"cell_type":"code","metadata":{"id":"7cD-gR88uB7r"},"source":["svc = Pipeline([\n","        (\"count_vectorizer\", CountVectorizer(analyzer = 'word',\n","                                             ngram_range=(3, 3), \n","                                             token_pattern=r'\\b\\w+\\b', \n","                                             min_df=1)),\n","        (\"linear svc\", SVC(kernel=\"rbf\")) # try with \"linear\", \"poly\", \"rbf\" or \"sigmoid\"\n","    ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMPVvf9DuHVz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627324285365,"user_tz":-120,"elapsed":73329,"user":{"displayName":"Santiago Arroniz Parra","photoUrl":"","userId":"14616025295717850812"}},"outputId":"ae7503e7-56b9-4e4c-d1ae-3550880c85dd"},"source":["model = svc\n","model.probability = True\n","model.fit(tweets_train, labels_train)\n","y_pred = model.predict(tweets_test)\n","print(accuracy_score(labels_test, y_pred))\n","print(classification_report(labels_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7457088366179275\n","              precision    recall  f1-score   support\n","\n","                   0.00      0.00      0.00         1\n","           1       0.88      0.23      0.36       496\n","           2       0.73      0.99      0.84      1076\n","\n","    accuracy                           0.75      1573\n","   macro avg       0.54      0.40      0.40      1573\n","weighted avg       0.78      0.75      0.69      1573\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"dBqoaup0Kso3"},"source":["Tampoco nos deja muy buenos resultados en comparacion con el modelo de unigrams. Intentemos con uno basado en caracteres:"]},{"cell_type":"markdown","metadata":{"id":"NqIwQMyeuO4A"},"source":["Parametro: **Character-gram**"]},{"cell_type":"code","metadata":{"id":"oebwDvOauVeC"},"source":["svc = Pipeline([\n","        (\"count_vectorizer\", CountVectorizer(analyzer = 'char',\n","                                             ngram_range=(3, 5))),\n","        (\"linear svc\", SVC(kernel=\"rbf\")) # try with \"linear\", \"poly\", \"rbf\" or \"sigmoid\"\n","    ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rkj4c1NKuWiI"},"source":["model = svc\n","model.probability = True\n","model.fit(tweets_train, labels_train)\n","y_pred = model.predict(tweets_test)\n","print(accuracy_score(labels_test, y_pred))\n","print(classification_report(labels_test, y_pred))"],"execution_count":null,"outputs":[]}]}